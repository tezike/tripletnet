# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/conditionalnet.ipynb (unless otherwise specified).

__all__ = ['resnet18', 'ConditionalSimNet']

# Cell
import numpy as np
import torch
import torch.nn as nn
import torchvision.models as models

# Cell
resnet18 = models.resnet18(pretrained=True)

# Cell
class ConditionalSimNet(nn.Module):
    def __init__(self, emb_extractor, num_sim_condition, embedding_size, learned_mask=True):
        """
        emb_extractor: base model that extracts the embeddings from the network
        num_sim_condition: number of similarity conditions being considered
        embedding_size: size of the mask. Must be that same size as the output from `emb_extractor`
        learned_mask: flag to know if the conditional mask should be learned/adjusted during training
        """

        super(ConditionalSimNet, self).__init__()
        self.emb_extractor = emb_extractor
        self.learned_mask = learned_mask

        if learned_mask:
            self.masks = nn.Embedding(num_sim_condition, embedding_size)
            #use a normal distribution to init the weights
            self.masks.weight.data.normal_(0.9, 0.7)

        else:
            self.masks = nn.Embedding(num_sim_condition, embedding_size)

            # use equal spacing in the embedding space to define the masks
            mask_array = np.zeros([n_conditions, embedding_size])
            mask_len = int(embedding_size / n_conditions)

            for i in range(n_conditions):
                mask_array[i, i*mask_len:(i+1)*mask_len] = 1
            # no gradients for the masks
            self.masks.weight = torch.nn.Parameter(torch.Tensor(mask_array), requires_grad=False)

    def forward(self, inp, sim_condition):
        embedding = self.emb_extractor(inp)
        mask = self.masks(sim_condition)
        if self.learned_mask:
            # set the embeddings to fall between 0 and 1
            masked_embedding = embedding * mask
        return masked_embedding, mask.norm(1), embedding.norm(2), masked_embedding.norm(2)